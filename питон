===========библиотечки
from sklearn.model_selection import KFold
from sklearn.datasets import make_moons
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
=====================================================МЕТОД К СОСЕДЕЙ
====создание некоторой двумерной выборки
X, y =make_moons(n_samples=1000, noise=0.5, random_state=10)
======РАЗДЕЛЕНИЕ НА ТЕСТОВУЮ И ТРЕНИРОВКУ 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=10)
plt.scatter(X[:, 0], X[:, 1], c=y)

========= ПРИКОЛ ПОЗВОЛЯЮЩИЙ ВЫБРАТЬ ЛУЧШИЕ ПАРАМЕТРЫ ДЛЯ МЕТОДА
grid_searcher = GridSearchCV(KNeighborsClassifier(),
                             param_grid={'n_neighbors': range(1, 20)
                                        },
                             cv=KFold(n_splits=5, random_state=10))
==ОБУЧАЕМ
grid_searcher.fit(X_train, y_train);
===ПРИМЕРЯЕМ 
pred = grid_searcher.predict(X_test)
==СМОТРИМ НА ЭФФЕКТИВНОСТЬ 
k=accuracy_score(pred, y_test )

================================================ ИМПОРТ КАРТИНОК 

from mnist import MNIST
from matplotlib import pyplot as plt
import numpy as np
mndata = MNIST('', gz=True)
images, labels = mndata.load_training()

# Далее мы берем только 5000 картинок и меток, чтобы не ждать обучения слишком долго:
images, labels = np.array(images)[:5000, :], np.array(labels)[:5000]

X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=10)
knn=KNeighborsClassifier(n_neighbors=30)
knn.fit(X_train, y_train)

pred = knn.predict(X_test)

from sklearn.metrics import accuracy_score
k=accuracy_score(pred, y_test)
k
